{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd450f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c963a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        probs = F.softmax(self.fc2(x), dim=-1)\n",
    "        return probs\n",
    "\n",
    "# Value function network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        value = self.fc2(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fff34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(values, masks, rewards, gamma=0.99, tau=0.95):\n",
    "    rewards = torch.cat(rewards)\n",
    "    values = torch.cat(values)\n",
    "    masks = torch.cat(masks)\n",
    "    targets = torch.zeros(rewards.size())\n",
    "    gae = torch.zeros(rewards.size())\n",
    "    for t in reversed(range(rewards.size(0))):\n",
    "        if t == rewards.size(0) - 1:\n",
    "            delta = rewards[t] - values[t]\n",
    "            gae[t] = delta\n",
    "        else:\n",
    "            delta = rewards[t] + gamma * values[t + 1] * masks[t + 1] - values[t]\n",
    "            gae[t] = delta + gamma * tau * masks[t + 1] * gae[t + 1]\n",
    "    targets = values + gae\n",
    "    return targets, (gae - gae.mean()) / (gae.std() + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69a6c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(Avp, b, nsteps=10, residual_tol=1e-10):\n",
    "    p = b.clone().detach()\n",
    "    r = b.clone().detach()\n",
    "    x = torch.zeros_like(b).detach()\n",
    "    rdotr = torch.dot(r, r)\n",
    "    for i in range(nsteps):\n",
    "        _tmp = Avp(p).detach()\n",
    "        alpha = rdotr / torch.dot(p, _tmp)\n",
    "        x += alpha * p\n",
    "        r -= alpha * _tmp\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        betta = new_rdotr / rdotr\n",
    "        p = r + betta * p\n",
    "        rdotr = new_rdotr\n",
    "        if rdotr < residual_tol:\n",
    "            break\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b2192a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(old_probs, new_probs):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between old and new probabilities.\n",
    "    \"\"\"\n",
    "    # Ensure the probabilities do not become 0 or 1\n",
    "    epsilon = 1e-5\n",
    "    old_probs = torch.clamp(old_probs, epsilon, 1 - epsilon)\n",
    "    new_probs = torch.clamp(new_probs, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return (old_probs * (torch.log(old_probs) - torch.log(new_probs))).sum(1, keepdim=True)\n",
    "\n",
    "def hessian_vector_product(tau, p_net, v, damping_factor=1e-2):\n",
    "    \"\"\"\n",
    "    Compute product of Hessian of KL divergence with vector v.\n",
    "    \"\"\"\n",
    "    kl = kl_divergence(tau, p_net(tau))\n",
    "    \n",
    "    # Compute the first derivative\n",
    "    grads = torch.autograd.grad(kl.sum(), p_net.parameters(), create_graph=True)\n",
    "    flat_grads = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "    # Compute the product of the first derivative with the vector v\n",
    "    grad_v_product = torch.sum(flat_grads * v)\n",
    "\n",
    "    # Compute the second derivative\n",
    "    hessian_v = torch.autograd.grad(grad_v_product, p_net.parameters())\n",
    "    hessian_v_product = torch.cat([grad.contiguous().view(-1) for grad in hessian_v]).data\n",
    "\n",
    "    return hessian_v_product + v * damping_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9902211a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linesearch(model, f, x, fullstep, expected_improve_rate, max_backtracks=10, accept_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Perform backtracking linesearch.\n",
    "    \"\"\"\n",
    "    fval = f(True).data\n",
    "    for step_frac in 0.5**np.arange(max_backtracks):\n",
    "        xnew = x + step_frac * fullstep\n",
    "        set_flat_params_to(model, xnew)\n",
    "        newfval = f(True).data\n",
    "        actual_improve = fval - newfval\n",
    "        expected_improve = expected_improve_rate * step_frac\n",
    "        ratio = actual_improve / expected_improve\n",
    "\n",
    "        if ratio.item() > accept_ratio and actual_improve.item() > 0:\n",
    "            return True, xnew\n",
    "    return False, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dcbc03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_flat_params_to(model, flat_params):\n",
    "    \"\"\"\n",
    "    Set the parameters of the model using a flat parameter tensor.\n",
    "    \"\"\"\n",
    "    prev_ind = 0\n",
    "    for param in model.parameters():\n",
    "        flat_size = int(np.prod(list(param.size())))\n",
    "        param.data.copy_(flat_params[prev_ind:prev_ind+flat_size].view(param.size()))\n",
    "        prev_ind += flat_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd328c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_objective(old_probs, new_probs, actions, advantages):\n",
    "    \"\"\"    \n",
    "    Compute the surrogate objective, which is used to derive the policy update.\n",
    "    \"\"\"\n",
    "    ratio = new_probs.gather(1, actions) / old_probs.gather(1, actions)\n",
    "    return (ratio * advantages).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec4dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_params_from(model):\n",
    "    \"\"\"\n",
    "    Retrieve the parameters from a model as a single flat tensor.\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        params.append(param.data.view(-1))\n",
    "    return torch.cat(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc67f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "in_dim = env.observation_space.shape[0]\n",
    "out_dim = env.action_space.n\n",
    "\n",
    "policy_net = PolicyNetwork(in_dim, out_dim)\n",
    "value_net = ValueNetwork(in_dim)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 1000\n",
    "max_steps = 200\n",
    "num_trajectories = 10\n",
    "gamma = 0.99\n",
    "tau = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed611079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 3. Compute the step direction using the conjugate gradient algorithm.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m hvp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: hessian_vector_product(torch\u001b[38;5;241m.\u001b[39mcat(states), policy_net, v)\n\u001b[0;32m---> 56\u001b[0m step_direction \u001b[38;5;241m=\u001b[39m conjugate_gradient(hvp, flat_policy_grad)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 4. Compute the step size using the line search.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m max_kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mconjugate_gradient\u001b[0;34m(Avp, b, nsteps, residual_tol)\u001b[0m\n\u001b[1;32m      5\u001b[0m rdotr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(r, r)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nsteps):\n\u001b[0;32m----> 7\u001b[0m     _tmp \u001b[38;5;241m=\u001b[39m Avp(p)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m      8\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m rdotr \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(p, _tmp)\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m p\n",
      "Cell \u001b[0;32mIn[21], line 55\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     52\u001b[0m flat_policy_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([grad\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m policy_grad])\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# 3. Compute the step direction using the conjugate gradient algorithm.\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m hvp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: hessian_vector_product(torch\u001b[38;5;241m.\u001b[39mcat(states), policy_net, v)\n\u001b[1;32m     56\u001b[0m step_direction \u001b[38;5;241m=\u001b[39m conjugate_gradient(hvp, flat_policy_grad)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 4. Compute the step size using the line search.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m, in \u001b[0;36mhessian_vector_product\u001b[0;34m(tau, p_net, v, damping_factor)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhessian_vector_product\u001b[39m(tau, p_net, v, damping_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    Compute product of Hessian of KL divergence with vector v.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     kl \u001b[38;5;241m=\u001b[39m kl_divergence(tau, p_net(tau))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Compute the first derivative\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     grads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(kl\u001b[38;5;241m.\u001b[39msum(), p_net\u001b[38;5;241m.\u001b[39mparameters(), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mkl_divergence\u001b[0;34m(old_probs, new_probs)\u001b[0m\n\u001b[1;32m      7\u001b[0m old_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(old_probs, epsilon, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon)\n\u001b[1;32m      8\u001b[0m new_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(new_probs, epsilon, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (old_probs \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mlog(old_probs) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(new_probs)))\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    cumulative_rewards = 0\n",
    "    for _ in range(num_trajectories):\n",
    "        state, _ = env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            probs = policy_net(state)\n",
    "            action = probs.multinomial(num_samples=1).detach()\n",
    "            next_state, reward, done, truncated, _ = env.step(action.numpy()[0][0])\n",
    "            done = done or truncated\n",
    "            \n",
    "            cumulative_rewards += reward\n",
    "            \n",
    "            log_prob = torch.log(probs.squeeze(0)[action])\n",
    "            entropy += -(log_prob * probs.squeeze(0)).sum()\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value_net(state))\n",
    "            rewards.append(torch.FloatTensor([reward]))\n",
    "            masks.append(torch.FloatTensor([1 - done]))\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "    next_value = value_net(next_state)\n",
    "    returns, advantages = get_advantages(values, masks, rewards, gamma, tau)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    \n",
    "    \n",
    "    # 1. Compute the surrogate objective.\n",
    "    old_probs = policy_net(torch.cat(states)).detach()\n",
    "    new_probs = policy_net(torch.cat(states))\n",
    "    surr_obj = surrogate_objective(old_probs, new_probs, torch.cat(actions), advantages)\n",
    "\n",
    "    # 2. Compute the gradient of the surrogate objective.\n",
    "    policy_net.zero_grad()\n",
    "    surr_obj.backward(retain_graph=True)\n",
    "    policy_grad = [param.grad for param in policy_net.parameters()]\n",
    "    flat_policy_grad = torch.cat([grad.view(-1) for grad in policy_grad])\n",
    "    \n",
    "    # 3. Compute the step direction using the conjugate gradient algorithm.\n",
    "    hvp = lambda v: hessian_vector_product(torch.cat(states), policy_net, v)\n",
    "    step_direction = conjugate_gradient(hvp, flat_policy_grad)\n",
    "\n",
    "    # 4. Compute the step size using the line search.\n",
    "    max_kl = 0.01\n",
    "    step_size = torch.sqrt(2 * max_kl / (torch.dot(step_direction, hvp(step_direction))))\n",
    "    full_step = step_size * step_direction\n",
    "\n",
    "    # 5. Perform line search to satisfy the KL constraint.\n",
    "    expected_improve = torch.dot(-full_step, policy_grad)\n",
    "    success, new_params = linesearch(policy_net, surrogate_objective, get_flat_params_from(policy_net), full_step, expected_improve)\n",
    "\n",
    "    if success:\n",
    "        set_flat_params_to(policy_net, new_params)\n",
    "\n",
    "    # Update the value function\n",
    "    values = torch.cat(values)\n",
    "    value_loss = (values - returns).pow(2).mean()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "    \n",
    "    avg_reward = cumulative_rewards / num_trajectories\n",
    "    print(f\"Epoch {epoch+1}, Average Reward: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419db7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
