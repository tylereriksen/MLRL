{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1486c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "def relu(mat):\n",
    "    return np.multiply(mat,(mat>0))\n",
    "    \n",
    "def relu_derivative(mat):\n",
    "    return (mat>0)*1\n",
    "\n",
    "class NNLayer:\n",
    "    # class representing a neural net layer\n",
    "    def __init__(self, input_size, output_size, activation=None, lr = 0.001):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.uniform(low=-0.5, high=0.5, size=(input_size, output_size))\n",
    "        self.stored_weights = np.copy(self.weights)\n",
    "        self.activation_function = activation\n",
    "        self.lr = lr\n",
    "        self.m = np.zeros((input_size, output_size))\n",
    "        self.v = np.zeros((input_size, output_size))\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.time = 1\n",
    "        self.adam_epsilon = 0.00000001\n",
    "\n",
    "    # Compute the forward pass for this layer\n",
    "    def forward(self, inputs, remember_for_backprop=True):\n",
    "        # inputs has shape batch_size x layer_input_size \n",
    "        \n",
    "        unactivated = None\n",
    "        if remember_for_backprop:\n",
    "            unactivated = np.dot(inputs, self.weights)\n",
    "            unactivated_with_bias = np.append(unactivated,1)\n",
    "        else: \n",
    "            unactivated = np.dot(inputs, self.stored_weights)\n",
    "            unactivated_with_bias = np.append(unactivated,1)\n",
    "        # store variables for backward pass\n",
    "        output = unactivated_with_bias\n",
    "        if self.activation_function != None:\n",
    "            # assuming here the activation function is relu, this can be made more robust\n",
    "            output = self.activation_function(output)\n",
    "        if remember_for_backprop:\n",
    "            self.backward_store_in = input_with_bias\n",
    "            self.backward_store_out = np.copy(unactivated)\n",
    "        return output    \n",
    "        \n",
    "    def update_weights(self, gradient):        \n",
    "        m_temp = np.copy(self.m)\n",
    "        v_temp = np.copy(self.v) \n",
    "        \n",
    "        m_temp = self.beta_1*m_temp + (1-self.beta_1)*gradient\n",
    "        v_temp = self.beta_2*v_temp + (1-self.beta_2)*(gradient*gradient)\n",
    "        m_vec_hat = m_temp/(1-np.power(self.beta_1, self.time+0.1))\n",
    "        v_vec_hat = v_temp/(1-np.power(self.beta_2, self.time+0.1))\n",
    "        self.weights = self.weights - np.divide(self.lr*m_vec_hat, np.sqrt(v_vec_hat)+self.adam_epsilon)\n",
    "        \n",
    "        self.m = np.copy(m_temp)\n",
    "        self.v = np.copy(v_temp)\n",
    "        \n",
    "    def update_stored_weights(self):\n",
    "        self.stored_weights = np.copy(self.weights)\n",
    "        \n",
    "    def update_time(self):\n",
    "        self.time = self.time+1\n",
    "        \n",
    "    def backward(self, gradient_from_above):\n",
    "        adjusted_mul = gradient_from_above\n",
    "        # this is pointwise\n",
    "        if self.activation_function != None:\n",
    "            adjusted_mul = np.multiply(relu_derivative(self.backward_store_out),gradient_from_above)\n",
    "        D_i = np.dot(np.transpose(np.reshape(self.backward_store_in, (1, len(self.backward_store_in)))), np.reshape(adjusted_mul, (1,len(adjusted_mul))))\n",
    "        delta_i = np.dot(adjusted_mul, np.transpose(self.weights))[:-1]\n",
    "        self.update_weights(D_i)\n",
    "        return delta_i\n",
    "        \n",
    "class RLAgent:\n",
    "    # class representing a reinforcement learning agent\n",
    "    env = None\n",
    "    def __init__(self, env, num_hidden_layers=2, hidden_size=24, gamma=0.95, epsilon_decay=0.997, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = env.observation_space.shape[0]\n",
    "        self.output_size = env.action_space.n\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.997\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = deque([],1000000)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.layers = [\n",
    "            NNLayer(self.input_size + 1, self.hidden_size, activation=relu),\n",
    "            NNLayer(self.hidden_size+1, self.hidden_size, activation=relu),\n",
    "            NNLayer(self.hidden_size+1, self.output_size)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "    def select_action(self, observation):\n",
    "        values = self.forward(np.asmatrix(observation))\n",
    "        if (np.random.random() > self.epsilon):\n",
    "            return np.argmax(values)\n",
    "        else:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "            \n",
    "    def forward(self, observation, remember_for_backprop=True):\n",
    "        vals = np.copy(observation)\n",
    "        index = 0\n",
    "        for layer in self.layers:\n",
    "            vals = layer.forward(vals, remember_for_backprop)\n",
    "            index = index + 1\n",
    "        return vals\n",
    "        \n",
    "    def remember(self, done, action, observation, prev_obs):\n",
    "        self.memory.append([done, action, observation, prev_obs])\n",
    "        \n",
    "    def experience_replay(self, update_size=20):\n",
    "        if (len(self.memory) < update_size):\n",
    "            return\n",
    "        else: \n",
    "            batch_indices = np.random.choice(len(self.memory), update_size)\n",
    "            for index in batch_indices:\n",
    "                done, action_selected, new_obs, prev_obs = self.memory[index]\n",
    "                action_values = self.forward(prev_obs, remember_for_backprop=True)\n",
    "                next_action_values = self.forward(new_obs, remember_for_backprop=False)\n",
    "                experimental_values = np.copy(action_values)\n",
    "                if done:\n",
    "                    experimental_values[action_selected] = -1\n",
    "                else:\n",
    "                    experimental_values[action_selected] = 1 + self.gamma*np.max(next_action_values)\n",
    "                self.backward(action_values, experimental_values)\n",
    "        self.epsilon = self.epsilon if self.epsilon < self.epsilon_min else self.epsilon*self.epsilon_decay\n",
    "        for layer in self.layers:\n",
    "            layer.update_time()\n",
    "            layer.update_stored_weights()\n",
    "        \n",
    "    def backward(self, calculated_values, experimental_values): \n",
    "        # values are batched = batch_size x output_size\n",
    "        delta = (calculated_values - experimental_values)\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "# Global variables\n",
    "NUM_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000\n",
    "AVERAGE_REWARD_TO_SOLVE = 195\n",
    "NUM_EPS_TO_SOLVE = 100\n",
    "NUM_RUNS = 20\n",
    "GAMMA = 0.95\n",
    "EPSILON_DECAY = 0.997\n",
    "update_size = 10\n",
    "hidden_layer_size = 24\n",
    "num_hidden_layers = 2\n",
    "model = RLAgent(env,num_hidden_layers,hidden_layer_size,GAMMA,EPSILON_DECAY)\n",
    "scores_last_timesteps = deque([], NUM_EPS_TO_SOLVE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2e85710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'input_with_bias' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iterating through time steps within an episode\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_TIMESTEPS):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# env.render()\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     prev_obs \u001b[38;5;241m=\u001b[39m observation\n\u001b[1;32m     14\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[17], line 102\u001b[0m, in \u001b[0;36mRLAgent.select_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m--> 102\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon):\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(values)\n",
      "Cell \u001b[0;32mIn[17], line 112\u001b[0m, in \u001b[0;36mRLAgent.forward\u001b[0;34m(self, observation, remember_for_backprop)\u001b[0m\n\u001b[1;32m    110\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 112\u001b[0m     vals \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremember_for_backprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     index \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vals\n",
      "Cell \u001b[0;32mIn[17], line 35\u001b[0m, in \u001b[0;36mNNLayer.forward\u001b[0;34m(self, inputs, remember_for_backprop)\u001b[0m\n\u001b[1;32m     33\u001b[0m unactivated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remember_for_backprop:\n\u001b[0;32m---> 35\u001b[0m     unactivated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[43minput_with_bias\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[1;32m     36\u001b[0m     input_with_bias \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(inputs,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'input_with_bias' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# The main program loop\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    observation, _ = env.reset()\n",
    "    rewards = []\n",
    "    if i_episode >= NUM_EPS_TO_SOLVE:\n",
    "        if (sum(scores_last_timesteps)/NUM_EPS_TO_SOLVE > AVERAGE_REWARD_TO_SOLVE):\n",
    "            print(\"solved after {} episodes\".format(i_episode))\n",
    "            break\n",
    "    # Iterating through time steps within an episode\n",
    "    for t in range(MAX_TIMESTEPS):\n",
    "        # env.render()\n",
    "        action = model.select_action(observation)\n",
    "        prev_obs = observation\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "        # Keep a store of the agent's experiences\n",
    "        model.remember(done, action, observation, prev_obs)\n",
    "        model.experience_replay(update_size)\n",
    "        # epsilon decay\n",
    "        if done:\n",
    "            # If the pole has tipped over, end this episode\n",
    "            print('Episode {} ended with a mean reward of {}'.format(i_episode, len(rewards)))\n",
    "            scores_last_timesteps.append(t+1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558e99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
